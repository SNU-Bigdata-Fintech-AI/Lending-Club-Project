{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b739aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost tqdm numpy_financial fredapi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc17fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, roc_curve, auc\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04320ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None) # ëª¨ë“  ì—´ ë‹¤ ë³´ê¸°\n",
    "pd.set_option('display.max_rows', None) # ëª¨ë“  í–‰ ë‹¤ ë³´ê¸°\n",
    "pd.set_option('display.width', None) # ì—´ ë„ˆë¹„ ë„‰ë„‰í•˜ê²Œ\n",
    "pd.set_option('display.max_colwidth', None) # ì—´ ì•ˆì˜ ê°’ ìë¥´ì§€ ì•Šê²Œ\n",
    "tqdm.pandas()\n",
    "\n",
    "# AWS S3ì—ì„œ ë°ì´í„° load\n",
    "url = \"https://snu-bigdata-fintech-ai.s3.ap-northeast-2.amazonaws.com/data/interim/preprocessed_data_ver.3.0.csv\"\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e5c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë…ë¦½ë³€ìˆ˜ì—ì„œ ì œì™¸í•  ë³€ìˆ˜ ëª©ë¡\n",
    "drop_cols = [\n",
    "    'term', 'last_pymnt_d', 'installment', 'funded_amnt',\n",
    "    'recoveries', 'collection_recovery_fee', 'default', 'issue_d',\n",
    "    'risk_free_rate', 'cash_flow', 'irr'\n",
    "]\n",
    "\n",
    "X = df.drop(columns=drop_cols)\n",
    "y = df['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcbd651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharpe ê³„ì‚° í•¨ìˆ˜\n",
    "def calculate_sharpe(returns, risk_free_rates):\n",
    "    excess = returns - risk_free_rates\n",
    "    if excess.std(ddof=1) == 0:\n",
    "        return -np.inf\n",
    "    return excess.mean() / excess.std(ddof=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed76cd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ë²”ìœ„ ì§€ì •\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'min_child_weight': [1, 3, 5],\n",
    "    'reg_alpha': [0, 0.1, 1],\n",
    "    'reg_lambda': [1, 5, 10]\n",
    "}\n",
    "\n",
    "# íŠœë‹ìš© ê³ ì •ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ í•˜ì´í¼íŒŒë¼ë¯¸í„° íƒìƒ‰ 1íšŒ ìˆ˜í–‰\n",
    "X_tune, _, y_tune, _ = train_test_split(X, y, test_size=0.3, stratify=y)\n",
    "\n",
    "model_base = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
    "search = RandomizedSearchCV(\n",
    "    model_base, param_distributions=param_dist, n_iter=9,\n",
    "    scoring='roc_auc', cv=2, n_jobs=1\n",
    ")\n",
    "search.fit(X_tune, y_tune)\n",
    "\n",
    "best_params = search.best_params_\n",
    "print(\"âœ… Best hyperparameters (from tuning):\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc7dd7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "best_models = []\n",
    "best_thresholds = []\n",
    "validation_sharpes = []\n",
    "test_sharpes = []\n",
    "test_approval_rates = []\n",
    "test_irr_means = []\n",
    "test_irr_positive_rates = []\n",
    "\n",
    "# í•´ë‹¹ best íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ 100ë²ˆ ë°˜ë³µ í•™ìŠµ ë° í‰ê°€\n",
    "for i in tqdm(range(100)):\n",
    "    # Train-test split\n",
    "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=i, stratify=y\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_temp, y_temp, test_size=0.25, random_state=i, stratify=y_temp\n",
    "    )\n",
    "\n",
    "    # best í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¡œ ëª¨ë¸ ìƒì„± ë° í•™ìŠµ\n",
    "    model = XGBClassifier(**best_params, eval_metric='logloss')\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡ ë° threshold íƒìƒ‰\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    thresholds = np.arange(0.0, 1.0, 0.05)\n",
    "\n",
    "    best_sharpe = -np.inf\n",
    "    best_threshold = None\n",
    "    val_indices = X_val.index\n",
    "    df_val = df.loc[val_indices]\n",
    "\n",
    "    for threshold in thresholds:\n",
    "        approved_mask = y_pred_proba <= threshold\n",
    "        denied_mask = ~approved_mask\n",
    "\n",
    "        selected = df_val.copy()\n",
    "        selected.loc[approved_mask, 'irr_adj'] = selected.loc[approved_mask, 'irr']\n",
    "        selected.loc[denied_mask, 'irr_adj'] = selected.loc[denied_mask, 'risk_free_rate']\n",
    "\n",
    "        returns = selected['irr_adj']\n",
    "        risk_free = selected['risk_free_rate']\n",
    "        valid = returns.notnull() & risk_free.notnull()\n",
    "\n",
    "        if valid.sum() < 2:\n",
    "            continue\n",
    "\n",
    "        sharpe = calculate_sharpe(returns[valid], risk_free[valid])\n",
    "\n",
    "        if sharpe > best_sharpe:\n",
    "            best_sharpe = sharpe\n",
    "            best_threshold = threshold\n",
    "\n",
    "    best_models.append(model)\n",
    "    best_thresholds.append(best_threshold)\n",
    "    validation_sharpes.append(best_sharpe)\n",
    "\n",
    "    # Test ë°ì´í„° í‰ê°€\n",
    "    y_test_proba = model.predict_proba(X_test)[:, 1]\n",
    "    test_approved_mask = y_test_proba <= best_threshold\n",
    "    df_test = df.loc[X_test.index]\n",
    "    test_selected = df_test[test_approved_mask]\n",
    "\n",
    "    returns_test = test_selected['irr']\n",
    "    risk_free_test = test_selected['risk_free_rate']\n",
    "    valid = returns_test.notnull() & risk_free_test.notnull()\n",
    "\n",
    "    returns_test = returns_test[valid]\n",
    "    risk_free_test = risk_free_test[valid]\n",
    "\n",
    "    sharpe_test = calculate_sharpe(returns_test, risk_free_test)\n",
    "    test_sharpes.append(sharpe_test)\n",
    "    test_approval_rates.append(len(returns_test) / len(df_test))\n",
    "    test_irr_means.append(returns_test.mean())\n",
    "    test_irr_positive_rates.append((returns_test > 0).mean())\n",
    "\n",
    "# ìµœì¢… ê²°ê³¼ ì¶œë ¥\n",
    "best_idx = np.argmax(test_sharpes)\n",
    "print(\"âœ… Best model index:\", best_idx)\n",
    "print(\"âœ… Best validation Sharpe ratio:\", validation_sharpes[best_idx])\n",
    "print(\"âœ… Best test Sharpe ratio:\", test_sharpes[best_idx])\n",
    "print(\"âœ… Best approval rate:\", test_approval_rates[best_idx])\n",
    "print(\"âœ… Mean IRR:\", test_irr_means[best_idx])\n",
    "print(\"âœ… Positive IRR ratio:\", test_irr_positive_rates[best_idx])\n",
    "print(\"âœ… Best threshold:\", best_thresholds[best_idx])\n",
    "print(\"âœ… Best model params:\", best_models[best_idx].get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cfecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best thresholds from 2 runs:\")\n",
    "print(best_thresholds)\n",
    "\n",
    "print(\"\\nValidation Sharpe Ratios from 2 runs:\")\n",
    "print(validation_sharpes)\n",
    "\n",
    "print(\"\\nTest Sharpe Ratios from 2 runs:\")\n",
    "print(test_sharpes)\n",
    "\n",
    "print(\"\\nTest Approval Rates:\")\n",
    "print(test_approval_rates)\n",
    "\n",
    "print(\"\\nTest IRR Means:\")\n",
    "print(test_irr_means)\n",
    "\n",
    "print(\"\\nTest IRR Positive Rates:\")\n",
    "print(test_irr_positive_rates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceb2297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ìµœì  ëª¨ë¸\n",
    "best_model = best_models[best_idx]\n",
    "\n",
    "# ê²€ì¦ ë°ì´í„°ì—ì„œ ì˜ˆì¸¡ í™•ë¥ \n",
    "y_val_proba = best_model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# --------------------------\n",
    "# 1. ROC Curve ì‹œê°í™”\n",
    "# --------------------------\n",
    "fpr, tpr, roc_thresholds = roc_curve(y_val, y_val_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.4f}\", color='blue')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', color='gray')\n",
    "plt.title(\"âœ… ROC Curve (Validation Set)\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 2. Thresholdë³„ Sharpe Ratio\n",
    "# --------------------------\n",
    "thresholds = np.arange(0.0, 1.0, 0.05)\n",
    "sharpe_ratios = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    approved_mask = y_val_proba <= threshold\n",
    "    denied_mask = ~approved_mask\n",
    "\n",
    "    selected = df_val.copy()\n",
    "    selected.loc[approved_mask, 'irr_adj'] = selected.loc[approved_mask, 'irr']\n",
    "    selected.loc[denied_mask, 'irr_adj'] = selected.loc[denied_mask, 'risk_free_rate']\n",
    "\n",
    "    returns = selected['irr_adj']\n",
    "    risk_free = selected['risk_free_rate']\n",
    "    valid = returns.notnull() & risk_free.notnull()\n",
    "\n",
    "    if valid.sum() < 2:\n",
    "        sharpe_ratios.append(np.nan)\n",
    "    else:\n",
    "        sharpe = calculate_sharpe(returns[valid], risk_free[valid])\n",
    "        sharpe_ratios.append(sharpe)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(thresholds, sharpe_ratios, marker='o', color='green')\n",
    "plt.title(\"âœ… Sharpe Ratio by Threshold (Validation Set)\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Sharpe Ratio\")\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c72d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ” IRR í‰ê·  ë° Positive ë¹„ìœ¨ ë¹„êµ\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.scatter(test_approval_rates, test_sharpes, color='purple')\n",
    "plt.title(\"ğŸ¯ Approval Rate vs Sharpe Ratio\")\n",
    "plt.xlabel(\"Approval Rate\")\n",
    "plt.ylabel(\"Test Sharpe Ratio\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4322df8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
